import numpy
import warnings

warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')

import jieba
import jieba.analyse
import pymysql
from gensim.models import word2vec
import re
from datetime import datetime, timedelta
import jieba.posseg as psg
import locale
import math
# æ˜¾ç¤ºæ— å‘å›¾
import networkx
import matplotlib.pyplot as plt
import numpy

from jpype import *


# =======================è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦==========================
# å»é™¤æ–‡æœ¬ä¸­çš„æ— æ„ä¹‰å­—ç¬¦
def cleanSen(str):
    str = str.strip(' ')
    str = str.replace('\\t', '')
    str = str.replace('\\n', '')
    str = str.replace('\\r', '')
    str = str.replace('\\u3000', '')
    return str


def extractKeywords(data):
    tfidf = jieba.analyse.extract_tags
    keywords = tfidf(data)
    return keywords


# è·å–å…³é”®è¯
def getKeywords(news, savePath):
    with open(savePath, 'w', encoding='UTF-8-SIG') as outf:
        keywords = extractKeywords(news)
        for word in keywords:
            outf.write(word + ' ')
        outf.write('\n')


# å…³é”®è¯å‘é‡åŒ–
def w2v(keyword, model):
    wordVec = numpy.zeros(50)
    for data in keyword:
        # print(data)
        data = data.split()
        first_word = data[0]
        if model.__contains__(first_word):
            wordVec = wordVec + model[first_word]
        for i in range(len(data) - 1):
            word = data[i + 1]
            # print(word)
            if model.__contains__(word):
                wordVec = wordVec + model[word]
    return wordVec


# è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
def similarity(vec1, vec2):
    vec1Mod = numpy.sqrt(sum(vec1**2))
    vec2Mod = numpy.sqrt(sum(vec2**2))
    if vec1Mod != 0 and vec2Mod != 0:
        similarity = (numpy.sum(vec1 * vec2)) / (vec1Mod * vec2Mod)
    else:
        similarity = 0
    return similarity


# ä¸¤ä¸ªæ–°é—»æ–‡æœ¬çš„ç›¸ä¼¼åº¦
def similarityOfText(news1, news2):
    # å»é™¤æ–‡æ¡£å¤šä½™å­—ç¬¦
    news1 = cleanSen(str(news1))
    news2 = cleanSen(str(news2))
    # è·å–å…³é”®è¯
    news1Keyword = extractKeywords(news1)
    news2Keyword = extractKeywords(news2)
    # news1Keyword = './simResult/news1Keyword.txt'
    # news2Keyword = './simResult/news2Keyword.txt'
    # ç”¨ word2vec è¿›è¡Œè®­ç»ƒ
    sentences = word2vec.Text8Corpus(u'./segmentResult/tmp.txt')
    # sentencesä¸ºè®­ç»ƒè¯­æ–™ min_countå°äºè¯¥æ•°çš„å•è¯ä¼šè¢«å‰”é™¤ï¼Œé»˜è®¤å€¼ä¸º5 windowsä¸ºç¥ç»ç½‘ç»œéšè—å±‚å•å…ƒæ•°ï¼Œé»˜è®¤100
    model = word2vec.Word2Vec(sentences, min_count=3, size=50, window=5, workers=4)
    # å…³é”®è¯å‘é‡åŒ–
    news1Vec = w2v(news1Keyword, model)
    news2Vec = w2v(news2Keyword, model)
    # ç›¸ä¼¼åº¦è®¡ç®—
    simOfText = similarity(news1Vec, news2Vec)
    return simOfText


# =======================è®¡ç®—æ—¶é—´ç›¸ä¼¼åº¦==========================
UTIL_CN_NUM = {
    'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4,
    'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
    '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,
    '5': 5, '6': 6, '7': 7, '8': 8, '9': 9
}
UTIL_CN_UNIT = {'å': 10, 'ç™¾': 100, 'åƒ': 1000, 'ä¸‡': 10000}


def cn2dig(src):
    if src == "":
        return None
    m = re.match("\d+", src)
    if m:
        return int(m.group(0))
    rsl = 0
    unit = 1
    for item in src[::-1]:
        if item in UTIL_CN_UNIT.keys():
            unit = UTIL_CN_UNIT[item]
        elif item in UTIL_CN_NUM.keys():
            num = UTIL_CN_NUM[item]
            rsl += num * unit
        else:
            return None
    if rsl < unit:
        rsl += unit
    return rsl


def year2dig(year):
    res = ''
    for item in year:
        if item in UTIL_CN_NUM.keys():
            res = res + str(UTIL_CN_NUM[item])
        else:
            res = res + item
    m = re.match("\d+", res)
    if m:
        if len(m.group(0)) == 2:
            return int(datetime.datetime.today().year / 100) * 100 + int(m.group(0))
        else:
            return int(m.group(0))
    else:
        return None


# åˆ©ç”¨jiebaåˆ†è¯ è¯†åˆ«å‡ºåˆ†è¯ç»“æœä¸­çš„æ•°å­—å’Œæ—¥æœŸè¯æ±‡ åªæå–åˆ°å¹´æœˆæ—¥
def time_extract(text):
    locale.setlocale(locale.LC_CTYPE, 'chinese')
    time_res = []
    word = ''
    keyDate = {'ä»Šå¤©': 0, 'æ˜å¤©': 1, 'åå¤©': 2}  # keyDate = {k:v}
    for k, v in psg.cut(text.strip()):
        if k in keyDate:
            if word != '':
                time_res.append(word)
            word = (datetime.today() + timedelta(days=keyDate.get(k, 0))).strftime('%Yå¹´%mæœˆ%dæ—¥')
        elif word != '':
            if v in ['m', 't']:
                word = word + k
            else:
                time_res.append(word)
                word = ''
        elif v in ['m', 't']:
            word = k
    if word != '':
        time_res.append(word)
    result = list(filter(lambda x: x is not None, [check_time_valid(w) for w in time_res]))
    final_res = [parse_datetime(w) for w in result]
    return [x for x in final_res if x is not None]


# å°†æ—¥æœŸä¸­ç»Ÿä¸€æ›¿æ¢æˆâ€œæ—¥â€
def check_time_valid(word):
    if 'å¹´' in word or 'æœˆ' in word or 'æ—¥' in word:
        m = re.match("\d+$", word)
        if m:
            if len(word) <= 6:
                return None
        word1 = re.sub('[å·|æ—¥]\d+$', 'æ—¥', word)
        if word1 != word:
            return check_time_valid(word1)
        else:
            return word1


# é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å¯¹æ—¥æœŸè¿›è¡Œåˆ‡åˆ†ï¼Œåˆ†ä¸ºå…·ä½“ç»´åº¦å†å¯¹å…·ä½“ç»´åº¦è¿›è¡Œè¯†åˆ«
def parse_datetime(msg):
    if msg is None or len(msg) == 0:
        return None
    # try:
    #     dt = parse(msg, fuzzy=True)
    #     return dt.strftime('%Y-%m-%d')
    # except Exception as e:
    else:
        m = re.match(
            r"([0-9é›¶ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å¹´)?([0-9ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æœˆ)?([0-9ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[å·æ—¥])?",
            msg)
        if m.group(0) is not None:
            res = {
                "year": m.group(1),
                "month": m.group(2),
                "day": m.group(3),
            }
            params = {}

            for name in res:
                if res[name] is not None and len(res[name]) != 0:
                    if name == 'year':
                        tmp = year2dig(res[name][:-1])
                    else:
                        tmp = cn2dig(res[name][:-1])
                    if tmp is not None:
                        params[name] = int(tmp)
            target_date = datetime.today().replace(**params)
            return target_date.strftime('%Y-%m-%d')
        else:
            return None


# å°†è·å–çš„æ—¶é—´è½¬æ¢æˆdatatimeå½¢å¼
def shiftDatatime(time):
    time = check_time_valid(time)
    time = parse_datetime(time)
    return time


# è®¡ç®—æ—¶é—´çš„ç›¸ä¼¼åº¦
def timeSimilarity(time1, time2, H):
    # å°†strè½¬åŒ–æˆdatatimeå†è¿›è¡Œè¿ç®—
    time1 = datetime.strptime(time1, '%Y-%m-%d')
    time2 = datetime.strptime(time2, '%Y-%m-%d')
    diff = time1 - time2
    simOfTime = -(math.log((abs(diff.days) + 1) / H) / math.log(H))
    return simOfTime


# ä¸¤ä¸ªæ–°é—»æ—¶é—´çš„ç›¸ä¼¼åº¦
def similarityOfTime(time1, time2, H):
    # é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æŠ½å–æ–‡æœ¬ä¸­çš„æ—¶é—´
    # print(news1, time_extract(news1), sep=':')
    # print(news2, time_extract(news2), sep=':')

    # å°†è·å–çš„æ—¶é—´è½¬æ¢æˆdatatimeå½¢å¼
    time1 = shiftDatatime(time1)
    # print(time1)
    time2 = shiftDatatime(time2)
    # print(time2)
    # è®¡ç®—æ—¶é—´çš„ç›¸ä¼¼åº¦
    simOfTime = timeSimilarity(time1, time2, H)
    return simOfTime


# ==========================è®¡ç®—å®ä½“ç›¸ä¼¼åº¦======================
# åœ°åè¯†åˆ«ï¼Œæ ‡æ³¨ä¸ºns
def Place_Recognize(sentence_str):
    HanLP = JClass('com.hankcs.hanlp.HanLP')
    segment = HanLP.newSegment().enablePlaceRecognize(True)
    return HanLP.segment(sentence_str)


# äººåè¯†åˆ«,æ ‡æ³¨ä¸ºnr
def PersonName_Recognize(sentence_str):
    HanLP = JClass('com.hankcs.hanlp.HanLP')
    segment = HanLP.newSegment().enableNameRecognize(True)
    return HanLP.segment(sentence_str)


# æœºæ„åè¯†åˆ«,æ ‡æ³¨ä¸ºnt
def Organization_Recognize(sentence_str):
    HanLP = JClass('com.hankcs.hanlp.HanLP')
    segment = HanLP.newSegment().enableOrganizationRecognize(True)
    return HanLP.segment(sentence_str)


# æ ‡æ³¨ç»“æœè½¬åŒ–æˆåˆ—è¡¨
def total_result(function_result_input):
    x = str(function_result_input)
    y = x[1:len(x) - 1]
    y = y.split(',')
    return y


# Type_Recognition å¯ä»¥é€‰ â€˜placeâ€™,â€˜personâ€™,â€˜organizationâ€™ä¸‰ç§å®ä½“,
# è¿”å›å•ä¸€å®ä½“ç±»åˆ«çš„åˆ—è¡¨
def single_result(Type_Recognition, total_result):
    if Type_Recognition == 'place':
        Type = '/ns'
    elif Type_Recognition == 'person':
        Type = '/nr'
    elif Type_Recognition == 'organization':
        Type = '/nt'
    else:
        print('è¯·è¾“å…¥æ­£ç¡®çš„å‚æ•°ï¼šï¼ˆplaceï¼Œpersonæˆ–organizationï¼‰')
    z = []
    for i in range(len(total_result)):
        if total_result[i][-3:] == Type:
            z.append(total_result[i])
    return z


# æŠŠå•ä¸€å®ä½“ç»“æœæ±‡æ€»æˆä¸€ä¸ªå­—å…¸
def dict_result(sentence):
    a = total_result(Place_Recognize(sentence))
    b = single_result('place', a)
    c = total_result(PersonName_Recognize(sentence))
    d = single_result('person', c)
    e = total_result(Organization_Recognize(sentence))
    f = single_result('organization', e)
    # g = total_result(NLP_tokenizer(sentence))
    # h = time_result(g)
    total_list = [i.strip() for i in b] + [i.strip() for i in d] + [i.strip() for i in f]
    # print('ä¸€ç»´æ•°ç»„å½¢å¼ï¼š', total_list)
    return total_list


# é™¤å»é‡å¤å®ä½“
def deleteRedundant(entityList):
    # total_list = {}.fromkeys(entityList).keys()
    total_list = list(set(entityList))
    return total_list


# æ±‚ä¸¤ä¸ªæ–°é—»å®ä½“çš„ç›¸ä¼¼åº¦
def entitySimilarity(entityOfNews1, entityOfNews2):
    # æ±‚ä¸¤ä¸ªæ•°ç»„çš„äº¤é›†
    intersection = list(set(entityOfNews1).intersection(set(entityOfNews2)))
    # print('å®ä½“äº¤é›†', intersection)
    # æ±‚ä¸¤ä¸ªæ•°ç»„çš„å¹¶é›†
    union = list(set(entityOfNews1).union(set(entityOfNews2)))
    # print('å®ä½“å¹¶é›†', union)
    simOfEntity = len(intersection) / len(union)
    return simOfEntity


# ä¸¤ä¸ªæ–°é—»çš„å®ä½“ç›¸ä¼¼åº¦
def similarityOfEntity(news1, news2):
    # startJVM(getDefaultJVMPath(), "-Djava.class.path=D:\hanlp\hanlp-portable-1.7.0.jar;D:\hanlp", "-Xms1g", "-Xmx1g")

    # è·å–æ–°é—»çš„å®ä½“åˆ—è¡¨ï¼Œä»¥ä¸€ç»´æ•°ç»„çš„å½¢å¼è¿”å›
    entityOfNews1 = dict_result(news1)
    entityOfNews2 = dict_result(news2)
    # å…³é—­JVMè™šæ‹Ÿæœº
    # shutdownJVM()
    # é™¤å»é‡å¤å®ä½“
    entityOfNews1 = deleteRedundant(entityOfNews1)
    # print('æ•°ç»„å»é™¤é‡å¤å®ä½“ï¼š', entityOfNews1)
    entityOfNews2 = deleteRedundant(entityOfNews2)
    # print('æ•°ç»„å»é™¤é‡å¤å®ä½“ï¼š', entityOfNews2)
    # æ±‚ä¸¤ä¸ªæ–°é—»å®ä½“çš„ç›¸ä¼¼åº¦
    simOfEntity = entitySimilarity(entityOfNews1, entityOfNews2)
    return simOfEntity


# ========================æ–°é—»ç›¸ä¼¼åº¦==========================
def similarityOfNews(news1, news2, time1, time2, H):
    # ä¸¤ä¸ªæ–°é—»æ–‡æœ¬ç›¸ä¼¼åº¦
    simOfText = similarityOfText(news1, news2)
    print('æ–‡æœ¬ç›¸ä¼¼åº¦ï¼š', simOfText)
    # ä¸¤ä¸ªæ–°é—»æ—¶é—´çš„ç›¸ä¼¼åº¦
    simOfTime = similarityOfTime(time1, time2, H)
    print('æ—¶é—´ç›¸ä¼¼åº¦ï¼š', simOfTime)
    # ä¸¤ä¸ªæ–°é—»çš„å®ä½“ç›¸ä¼¼åº¦
    simOfEntity = similarityOfEntity(news1, news2)
    print('å®ä½“ç›¸ä¼¼åº¦ï¼š', simOfEntity)
    # äº‹ä»¶çš„ç›¸ä¼¼åº¦
    simeos = simOfText * simOfTime * simOfEntity
    print('äº‹ä»¶ç›¸ä¼¼åº¦ï¼š', simeos)
    return simeos


# æ˜¾ç¤ºæ— å‘å›¾
def graphShow(G, adjacencyMatrix):
    matrix = numpy.array(adjacencyMatrix)
    for i in range(len(matrix)):
        for j in range(i + 1, len(matrix)):
            if matrix[i][j] == 1:
                G.add_edge(i, j)
    networkx.draw(G)
    plt.show()
    return


# Tanimotoç³»æ•°(å¹¿ä¹‰jaccardç›¸ä¼¼åº¦)
def Tanimoto(vec1, vec2):
    vec1Mod = numpy.sqrt(sum(vec1**2))
    vec2Mod = numpy.sqrt(sum(vec2**2))
    similarity = (vec1.dot(vec2)) / (vec1Mod + vec2Mod - vec1.dot(vec2))
    return similarity


# jaccardç›¸ä¼¼åº¦
def jaccard(news1, news2):
    # å»é™¤æ–‡æ¡£å¤šä½™å­—ç¬¦
    news1 = cleanSen(str(news1))
    news2 = cleanSen(str(news2))
    # è·å–å…³é”®è¯
    news1Keyword = extractKeywords(news1)
    news2Keyword = extractKeywords(news2)
    # ç”¨ word2vec è¿›è¡Œè®­ç»ƒ
    sentences = word2vec.Text8Corpus(u'./segmentResult/tmp.txt')
    # sentencesä¸ºè®­ç»ƒè¯­æ–™ min_countå°äºè¯¥æ•°çš„å•è¯ä¼šè¢«å‰”é™¤ï¼Œé»˜è®¤å€¼ä¸º5 windowsä¸ºç¥ç»ç½‘ç»œéšè—å±‚å•å…ƒæ•°ï¼Œé»˜è®¤100
    model = word2vec.Word2Vec(sentences, min_count=3, size=50, window=5, workers=4)
    # å…³é”®è¯å‘é‡åŒ–
    news1Vec = w2v(news1Keyword, model)
    news2Vec = w2v(news2Keyword, model)
    # å¹¿ä¹‰Jaccardç›¸ä¼¼åº¦è®¡ç®—
    simOfText = Tanimoto(news1Vec, news2Vec)
    return simOfText


# æŠ›å…‰å›¾çš„è¿‡ç¨‹
def polishGraph(G, textOfRelatedNews, P):
    VNeighbor = []
    for v in G.nodes():
        # æ±‚æ¯ä¸ªç‚¹çš„é‚»è¿‘ç‚¹N(v)
        NV = G.neighbors(v)
        # å°†æ¯ä¸ªç‚¹çš„é‚»è¿‘ç‚¹å­˜å…¥æ•°ç»„
        # è¿­ä»£å™¨éå†ï¼Œåªå‰è¿›ä¸åé€€
        for i in NV:
            VNeighbor.append(i)

        for u in VNeighbor:
            minJaccard = 0
            maxJaccard = 0
            UNeighbor = []
            # æ±‚æ¯ä¸ªé‚»è¿‘ç‚¹çš„é‚»è¿‘ç‚¹
            NU = G.neighbors(u)
            # å°†æ¯ä¸ªé‚»è¿‘ç‚¹çš„é‚»è¿‘ç‚¹å­˜å‚¨åˆ°æ•°ç»„ä¸­
            # è¿­ä»£å™¨éå†ï¼Œåªå‰è¿›ä¸åé€€
            for i in NU:
                UNeighbor.append(i)
            # è®¡ç®—å…¬å¼f(u, v)
            # è®¡ç®—ANeighborä¸BNeighborçš„äº¤é›†
            intersection = list(set(VNeighbor).intersection(set(UNeighbor)))
            if len(intersection) != 0:
                # è®¡ç®—ANeighborä¸BNeighborçš„å¹¶é›†
                union = list(set(VNeighbor).union(set(UNeighbor)))
                for n in union:
                    # max = 0
                    # æ±‚max{J(u, n), J(v, n)}
                    # J(A, B)ä¸ºå¹¿ä¹‰Jaccardç›¸ä¼¼åº¦ EJ(A,B)=(A*B)/(||A||^2+||B||^2-A*B)
                    jaccardUN = jaccard(textOfRelatedNews[u], textOfRelatedNews[n])
                    jaccardVN = jaccard(textOfRelatedNews[v], textOfRelatedNews[n])
                    maxJaccard += max(jaccardUN, jaccardVN)
                    if n in intersection:
                        minJaccard += min(jaccardUN, jaccardVN)
                print('~~~~~~~~~~~~~~~~~~~~~~~~~')
                print('minï¼š', minJaccard)
                print('maxï¼š', maxJaccard)
                fuv = minJaccard/maxJaccard
                print('f(u,v):', fuv)
                # è‹¥f(u,v)å¤§äºé˜ˆå€¼åˆ™æ·»åŠ uvä¹‹é—´çš„è¾¹ï¼Œå¦åˆ™åˆ é™¤uvä¹‹é—´çš„è¾¹
                if fuv >= -1:
                    P[u][v] = 1
    print('polish over')
    return


# ========================ä¸»å‡½æ•°==========================
def main():
    # connect mysql
    # conn = pymysql.connect(
    #     host='localhost',  # mysqlæœåŠ¡å™¨åœ°å€
    #     user='root',  # ç”¨æˆ·å
    #     passwd='root',  # å¯†ç 
    #     db='renminnews',  # æ•°æ®åº“
    #     charset='utf8'
    # )
    # # è·å–æ¸¸æ ‡
    # cursor = conn.cursor()
    # # ä»æ•°æ®åº“è·å–ä¸¤ç¯‡æ–°é—»
    # sql1 = "SELECT content FROM news WHERE title='æ„å‘è‹èµ”æ¬¾ä¸€ä¸‡ä¸‡ç¾å…ƒ'"
    # sql2 = "SELECT content FROM news WHERE title='è‹è”é’¢é“ç”Ÿäº§è¶…è¿‡è®¡åˆ’'"
    # try:
    #     cursor.execute(sql1)
    #     news1 = cursor.fetchall()
    #     cursor.execute(sql2)
    #     news2 = cursor.fetchall()
    # except:
    #     print("select is failed")
    # cursor.close()
    # conn.close()

    # ä»testDataæ–‡ä»¶ä¸­è¯»å–æ–°é—»
    newsList = []
    file = open("./test/testData.txt", encoding='UTF-8-SIG')
    for line in file:
        newsList.append(line)

    # ä»testTimeæ–‡ä»¶ä¸­è¯»å–å¯¹åº”æ–°é—»çš„æ—¶é—´
    timeList = []
    file = open("./test/testTime.txt", encoding='UTF-8-SIG')
    for line in file:
        timeList.append(line)

    # ç»™å®šçš„æ–°é—»new1
    news1 = "å¼ºçƒˆåœ°éœ‡ååå‡ åˆ†é’Ÿï¼Œå¤§åœ°è¿˜åœ¨æŠ–åŠ¨ï¼Œåˆšåˆšè„±é™©çš„å¼€æ»¦å”å±±ç…¤çŸ¿å·¥ä¼šå‰¯ä¸»ä»»æç‰æ—å’Œä»–çš„æˆ˜å‹ä»¬ï¼Œå¸¦ç€å”å±±å¸‚å‡ åä¸‡äººæ°‘çš„è¿«åˆ‡å¿ƒæƒ…ï¼Œé©¾é©¶ç€ä¸€è¾†çŸ¿å±±æ•‘æŠ¤è½¦ï¼ŒæœåŒ—äº¬æ–¹å‘å¼€å»ï¼Œå»æŠ¥å‘Šå…šä¸­å¤®ï¼ŒæŠ¥å‘Šæ¯›ä¸»å¸­ã€‚ä»–ä»¬éƒ½æœ‰å®¶ï¼Œæ­¤åˆ»ï¼Œè°éƒ½æ²¡æœ‰æƒ³åˆ°è‡ªå·±çš„å®¶ï¼Œè°éƒ½æ²¡æœ‰é¡¾åŠè‡ªå·±çš„å®¶ã€‚ç©¶ç«Ÿæ˜¯ä»€ä¹ˆç²¾ç¥åŠ›é‡æŒ‡æŒ¥ç€ä»–ä»¬è¿™ä¸€è‹±å‹‡çš„è¡ŒåŠ¨ï¼Ÿæç‰æ—æ¿€æƒ…æ»¡æ€€åœ°å›ç­”è¯´ï¼Œâ€œæŠ—ç¾å…¨é å…šæŒ‡å¼•ï¼Œçº¢å¿ƒå‘ç€æ¯›ä¸»å¸­ï¼â€"
    time1 = "1976å¹´8æœˆ10æ—¥"
    H = 34

    # è®¾ ğœƒ1ä¸º0,å¤§äºğœƒ1åˆ™è®¤ä¸ºæ˜¯ä¸ç»™å®šçš„news1ç›¸å…³çš„æ–°é—»
    textOfRelatedNews = []
    timeOfRelatedNews = []

    startJVM(getDefaultJVMPath(), "-Djava.class.path=D:\hanlp\hanlp-portable-1.7.0.jar;D:\hanlp", "-Xms1g", "-Xmx1g")
    for i in range(0, len(newsList)):
        news2 = newsList[i].replace("\n", "")
        print(news2)
        time2 = timeList[i].replace("\n", "")
        print(time2)
        simeos = similarityOfNews(news1, news2, time1, time2, H)
        # è®¾ ğœƒ1ä¸º0,å¤§äºğœƒ1åˆ™è®¤ä¸ºæ˜¯ä¸ç»™å®šçš„news1ç›¸å…³çš„æ–°é—»,ç›¸å…³æ–°é—»é›†åˆC
        if simeos > 0.0:
            textOfRelatedNews.append(news2)
            timeOfRelatedNews.append(time2)
    print('=====================================')
    # é›†åˆCçš„æ–°é—»ç›¸äº’è®¡ç®—ç›¸ä¼¼åº¦ï¼Œè®¾ ğœƒ2ä¸º0.1ï¼Œå¤§äºğœƒ2æ„æˆå›¾
    G = networkx.Graph()
    # é‚»æ¥çŸ©é˜µå­˜å‚¨æ— å‘å›¾
    adjacencyMatrix = [[0] * len(textOfRelatedNews) for x in range(len(textOfRelatedNews))]
    # å­˜å‚¨æŠ›å…‰åçš„å›¾
    P = [[0] * len(textOfRelatedNews) for x in range(len(textOfRelatedNews))]
    for i in range(0, len(textOfRelatedNews)):
        news1 = textOfRelatedNews[i]
        time1 = timeOfRelatedNews[i]
        for j in range(i + 1, len(textOfRelatedNews)):
            news2 = textOfRelatedNews[j]
            time2 = timeOfRelatedNews[j]
            simeos = similarityOfNews(news1, news2, time1, time2, H)
            print('(', i, j, ')=', simeos)
            if simeos > 0.1:
                adjacencyMatrix[i][j] = 1
                adjacencyMatrix[j][i] = 1
    shutdownJVM()
    # æ˜¾ç¤ºæ— å‘å›¾
    graphShow(G, adjacencyMatrix)
    # åŠ æƒå¾®èšç±»ç®—æ³•è¿›è¡Œæ— å‘å›¾æŠ›å…‰
    polishGraph(G, textOfRelatedNews, P)
    # æ˜¾ç¤ºæŠ›å…‰åçš„æ— å‘å›¾
    graphShow(G, P)
    #


if __name__ == "__main__":
    main()
